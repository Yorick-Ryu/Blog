---
title: 机器学习与深度学习复习
tags:
  - AI
  - ML
  - DL
  - 课程笔记
index_img: /img/default.png
categories:
  - AI
date: 2023-06-01 18:57:34
sticky:
password: 20230608
abstract: 请输入密码！提示：密码是考试日期，如：20220202
message: 请输入密码！提示：密码是考试日期，如：20220202
wrong_pass_message: 抱歉, 这个密码看着不太对, 请再试试。
wrong_hash_message: 抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容。
---

# 机器学习与深度学习复习

## （1）机器学习的含义。



（2）机器学习的英文翻译。
（3）机器学习的基本流程。
（4）交叉验证的三种方式。
（5）机器学习中的数据集的划分以及划分的意义。
（6）数据标准化的意义和两种不同的方法。
（7）梯度下降的三种方式。
（8）模型过（欠）拟合的含义以及如何避免。
（9）逻辑回归的基本含义。
（10）线性回归算法的基本步骤。

## （11）决策树算法中生成决策树的三种算法。

决策树的构建算法主要有**ID3、C4.5、 CART**三种，其中ID3和C4.5是分类树，CART是分类回归树。ID3是决策树最基本的构建算法，C4.5和CART是在ID3的基础上进行的优化算法。

ID3、C4.5和CART是三种常见的决策树构建算法，它们的主要区别如下：

1. ID3算法：ID3是“Iterative Dichotomiser 3”的缩写，它是一种基于信息增益的决策树构建算法。ID3算法会选择最大信息增益的特征作为节点进行划分，但它不能处理连续型特征和缺失值。

2. C4.5算法：C4.5是ID3算法的改进版，它可以处理连续型特征和缺失值。C4.5算法使用信息增益比来选择最佳特征进行划分，这个比值可以解决ID3算法中信息增益偏向于取值比较多的特征的问题。

3. CART算法：CART是“Classification and Regression Trees”的缩写，它既可以用于分类问题，也可以用于回归问题。CART算法使用基尼系数来选择最佳特征进行划分，基尼系数可以度量数据集的不纯度。CART算法可以处理连续型特征，但不能处理缺失值。

总的来说，ID3、C4.5和CART算法都是基于决策树的构建算法，它们在特征选择、处理连续型特征和缺失值等方面有所不同，选择合适的算法取决于数据集的特征和问题的类型。

**信息增益是什么？**

**信息增益**是指在决策树算法中，一个特征对于分类结果的贡献程度。它是通过计算某个特征对于数据集分类的不确定性减少程度来衡量的。具体来说，信息增益越大，说明该特征对于分类结果的影响越大，应该优先选择该特征作为节点划分的依据。

举个例子，假设有一个数据集包含以下几个样本：

| 序号 | 年龄 | 性别 | 是否有工作 | 是否有房子 | 是否有车 | 是否有存款 | 是否信贷成功 |
| ---- | ---- | ---- | ---------- | ---------- | -------- | ---------- | ------------ |
| 1    | 青年 | 男   | 否         | 否         | 否       | 否         | 否           |
| 2    | 青年 | 男   | 否         | 否         | 否       | 是         | 否           |
| 3    | 青年 | 女   | 否         | 否         | 否       | 是         | 是           |
| 4    | 青年 | 女   | 是         | 否         | 否       | 是         | 是           |
| 5    | 青年 | 女   | 是         | 是         | 否       | 否         | 是           |
| 6    | 中年 | 女   | 是         | 是         | 否       | 否         | 是           |
| 7    | 中年 | 女   | 是         | 是         | 是       | 否         | 是           |
| 8    | 中年 | 男   | 否         | 是         | 是       | 否         | 否           |
| 9    | 中年 | 女   | 是         | 否         | 是       | 是         | 是           |
| 10   | 老年 | 女   | 是         | 否         | 是       | 是         | 是           |

其中，最后一列是分类结果，表示是否信贷成功，其他列是特征。

假设我们想要用年龄这个特征来划分节点，计算信息增益的步骤如下：

1. 计算数据集的熵，即所有样本的分类结果不确定性，公式为 $H(D) = -\sum_{i=1}^{n}p_i\log_2p_i$，其中 $n$ 表示分类结果的种类数，$p_i$ 表示第 $i$ 种分类结果的概率。在这个例子中，分类结果只有两种，即信贷成功和不成功，因此 $n=2$，$p_1$ 表示信贷成功的概率，$p_2$ 表示不成功的概率。根据上表，$p_1=7/10$，$p_2=3/10$，因此 $H(D)=-0.7\log_2{0.7}-0.3\log_2{0.3}=0.881$。

2. 计算以年龄为特征划分节点后的条件熵，即根据年龄的不同取值将数据集划分成若干个子集，计算每个子集的熵的加权平均值，公式为 $H(D|A) = \sum_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i)$，其中 $n$ 表示年龄的取值个数，$D_i$ 表示年龄为第 $i$ 种取值的样本子集，$|D_i|$ 表示子集大小，$|D|$ 表示数据集大小。在这个例子中，年龄的取值有三种，即青年、中年、老年，因此 $n=3$。根据上表，年龄为青年的样本子集有5个，其中有2个信贷成功，3个不成功，因此 $H(D_1)=-\frac{2}{5}\log_2{\frac{2}{5}}-\frac{3}{5}\log_2{\frac{3}{5}}=0.971$。同理，年龄为中年的样本子集有4个，其中有3个信贷成功，1个不成功，因此 $H(D_2)=-\frac{3}{4}\log_2{\frac{3}{4}}-\frac{1}{4}\log_2{\frac{1}{4}}=0.811$。年龄为老年的样本子集有1个，信贷成功，因此 $H(D_3)=0$。因此，$H(D|A)=\frac{5}{10} \times 0.971 + \frac{4}{10} \times 0.811 + \frac{1}{10} \times 0 = 0.834$。

3. 计算信息增益，即数据集的熵减去以年龄为特征划分节点后的条件熵，公式为 $Gain(D,A) = H(D) - H(D|A)$。在这个例子中，$Gain(D,A) = 0.881 - 0.834 = 0.047$。因此，年龄这个特征对于分类结果的贡献较小，不是一个好的节点划分依据。



（12）分类算法性能评价几种常见的评价指标。
（13）随机森林算法的基础算法。
（14）支持向量机算法中的几种常见核函数。
（15）KNN算法的名称。
（16）集成学习算法的含义及基本思想。
（17）猫狗分类器中，准确率（Accuracy）、精准率（Precision）、召回率（Recall）、F1值（F1-Score）如何计算。

## （18）根据给定的TP/FP/FN/TN，计算准确率、精准率、召回率、F1值。

混淆矩阵是评估分类模型性能的一种常用方法，它将模型预测结果与真实结果进行比较，得到分类结果的准确性、召回率、精确率等指标。

以下是一个二分类问题的混淆矩阵示例：

| 真实结果/预测结果 | 正例         | 反例         |
| ----------------- | ------------ | ------------ |
| 正例              | TP（真正例） | FN（假反例） |
| 反例              | FP（假正例） | TN（真反例） |

其中，TP表示模型将正例正确地预测为正例的数量，FN表示模型将正例错误地预测为反例的数量，FP表示模型将反例错误地预测为正例的数量，TN表示模型将反例正确地预测为反例的数量。

例如，在一个肿瘤诊断的二分类问题中，正例表示患有肿瘤，反例表示没有肿瘤。如果模型将一个患有肿瘤的病人预测为没有肿瘤，那么这个样本就被归类为假反例（FN）。如果模型将一个没有肿瘤的病人预测为患有肿瘤，那么这个样本就被归类为假正例（FP）。

通过混淆矩阵可以计算出模型的准确率、召回率、精确率等指标，进而评估模型的性能和优化模型的参数。

## （19）信息熵如何计算。

在信息论中，熵（Entropy）是一个衡量随机变量不确定性的度量。在机器学习中，熵被广泛应用于决策树算法中的信息增益计算。

信息增益是指在决策树算法中，某个特征对分类结果的影响程度。信息增益越大，说明该特征对分类结果的影响越大，因此在决策树的分裂过程中应该优先选择这个特征。

熵的计算公式如下：

$H(X)=-\sum_{i=1}^{n}p_i\log_2p_i$

其中，$X$是一个随机变量，$p_i$表示该随机变量取值为$i$的概率。

熵的值越大，表示该随机变量的不确定性越高，因为它有更多的取值可能性。当随机变量的取值只有一个时，熵为0，表示该随机变量的取值是确定的。当随机变量的取值具有相同的概率分布时，熵达到最大值，此时所有可能的取值都是等可能的。

在决策树算法中，我们可以使用熵来计算每个特征对应的信息增益，从而选择最优的特征进行分裂。

（20）欧式距离和曼哈顿距离如何计算。
（21）使用Max-Min标准化方法将数据集标准化。
（22）贝叶斯算法的核心思想。
（23）层次聚类的2种类别。
（24）有监督和无监督算法的含义是什么。
（25）理解线性回归、逻辑回归算法、决策树和贝叶斯程序（代码），重点理解哪些语句是进行算法模型训练（拟合），哪些语句是进行数据预测的。
（26）全连接神经网络的含义。